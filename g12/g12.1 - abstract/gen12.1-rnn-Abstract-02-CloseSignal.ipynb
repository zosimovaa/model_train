{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Постановка задачи\n",
    "\n",
    "Тему с RNN не пустил в ход, т.к. надо переделывать sample и batch конвертеры на RNN - добавлять еще  один дименшн.\n",
    "По хорошему надорефаакторить в отдельный компонент и импортировать его в плеер и в агента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen12.1-rnn-Abstract-02-CloseSignal\n"
     ]
    }
   ],
   "source": [
    "# Системные импорты и настройки\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import random\n",
    "import warnings\n",
    "import ipynbname\n",
    "import logging.config\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# for local development\n",
    "RT_LIBS_PATH = \"/Users/alex/Dev_projects/MyOwnRepo/rt_libs/src\"\n",
    "BA_LIBS_PATH = \"/Users/alex/Dev_projects/MyOwnRepo/basic_application/src\"\n",
    "sys.path.append(RT_LIBS_PATH)\n",
    "sys.path.append(BA_LIBS_PATH)\n",
    "\n",
    "# read config\n",
    "with open('config.yaml', \"r\") as stream:\n",
    "    config = yaml.safe_load(stream)\n",
    "    \n",
    "# set logging config\n",
    "log_config = config.get(\"log\", None)\n",
    "logging.config.dictConfig(log_config)\n",
    "\n",
    "# set notebook alias\n",
    "ALIAS = ipynbname.name()\n",
    "print(ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DS frameworks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 23:37:54.440486: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "# NN Frameworks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D, AveragePooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.keras.models import load_model, clone_model\n",
    "\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RT packages\n",
    "from rl import DQNAgent\n",
    "from env import TradeEnv\n",
    "from core import ConstructorGen2\n",
    "\n",
    "from data_point import DataPointFactory\n",
    "\n",
    "from train_tools import Player, plot_and_go\n",
    "\n",
    "from train_tools import Player, plot_and_go\n",
    "from train_tools.train_plot import TrainPlot4\n",
    "from train_tools.train_manager import TrainManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value= 0\n",
    "#os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "#np.random.seed(seed_value)\n",
    "#tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конфиг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_len = 1\n",
    "\n",
    "# Параметры точки наблюдения\n",
    "observation_config = {\n",
    "    \"observation_len\": observation_len,             # Количество точек наблюдения в сэмпле\n",
    "    \"offset\": observation_len,                      # Количество точек наблюдения в сэмпле\n",
    "    \"future_points\": 0,                             # Количество будущех точек для предсказания тренда (временное решение)\n",
    "    \"step_size\": 1,                                 # Шаг по датасету\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "n_steps = 100\n",
    "sample_num = 10\n",
    "sample_size = n_steps//sample_num\n",
    "safe_interval_size = 3\n",
    "\n",
    "dataset = np.empty(0)\n",
    "\n",
    "sample_size_ = sample_size - safe_interval_size\n",
    "for i in range(sample_num):\n",
    "    safe_interval = np.zeros(safe_interval_size)\n",
    "    total = 0\n",
    "    while not total==1:\n",
    "        sample = np.random.uniform(size=sample_size_) > 0.9\n",
    "        total = sum(sample)\n",
    "\n",
    "    dataset = np.concatenate([dataset, sample, safe_interval])\n",
    "    \n",
    "data_train = pd.DataFrame(dataset, columns=[\"close_signal\"], dtype=np.int8)\n",
    "\n",
    "#data_train['close_signal'] = data_train['close_signal'].replace(0, -1)\n",
    "#data_train['close_signal'] = data_train['close_signal'].replace(1, 0.5)\n",
    "#data_train['close_signal']  = data_train['close_signal']  - 0.5\n",
    "\n",
    "print(data_train[\"close_signal\"].sum())\n",
    "print(data_train[\"close_signal\"].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициализация компонентов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datapoint factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpf_train = DataPointFactory(dataset=data_train, **observation_config)\n",
    "dpf_test = DataPointFactory(dataset=data_train, **observation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_config = {\n",
    "    \"context\":{\"class\": \"Context\", \"params\":{}},\n",
    "    \"action_controller\":{\n",
    "        \"class\": \"AbstractCloseSignal\",\n",
    "        \"params\":{ \n",
    "            \"penalty\": -2, \n",
    "            \"reward\": 0,\n",
    "        },\n",
    "    },\n",
    "    \"observation_builder\":{\n",
    "        \"class\": \"ObservationBuilderInput\",\n",
    "        \"inputs\": [\n",
    "            {\"class\": \"Input1D\", \"features\": [\n",
    "                {\"class\": \"TradeStateFeature\", \"params\": {}},\n",
    "                {\"class\": \"RawValueFeature1D\", \"params\": {\"name\": \"close_signal\"}}\n",
    "            ]}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "core_constructor = ConstructorGen2()\n",
    "env_core = core_constructor.get_core(core_config)\n",
    "\n",
    "# train environment\n",
    "env = TradeEnv(env_core, dpf_train, alias=ALIAS, log=False, log_obs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1, 2)]            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 8)                 352       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 388\n",
      "Trainable params: 388\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 23:37:59.570122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "ACTIVATION = 'relu'\n",
    "def create_q_model(env):\n",
    "    num_actions = env.action_space\n",
    "    #----------------------------------------------\n",
    "    \n",
    "    inp_static = Input(shape=(1, env.observation_space[0]))\n",
    "    classif = LSTM(8, activation=ACTIVATION)(inp_static)\n",
    "\n",
    "    output = Dense(num_actions, activation='softmax')(classif)\n",
    "\n",
    "    model = Model(inputs=inp_static, outputs=output)\n",
    "    return model\n",
    "\n",
    "model = create_q_model(env)\n",
    "model_target = create_q_model(env)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "next_state, reward, done, _ = agent.env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_tensor = agent._sample_transformer(next_state)\n",
    "state_tensor = np.expand_dims(next_state, (0,1))\n",
    "state_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Реализация DQN-агента.\n",
    "В этой верси все обновления (тренировка сети, параметры алгоритма) привязаны к фреймам.\n",
    "\n",
    "\"\"\"\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "\n",
    "from rl.components.replay_buffer import ReplayBuffer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    EXPORT_FORBIDDEN_ATTRIBUTES = (\"model\", \"model_target\", \"optimizer\")\n",
    "\n",
    "    def __init__(self, env, model, model_target):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.model_target = model_target\n",
    "\n",
    "        self.state_size = env.observation_space\n",
    "        self.action_size = env.action_space\n",
    "\n",
    "        # decay or discount rate: enables agent to take into account future actions in addition to the immediate ones, but discounted at this rate\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        # Number of frames for exploration\n",
    "        self.epsilon_random_frames = 5000\n",
    "        self.__epsilon_greedy_frames = 100000\n",
    "\n",
    "        # Epsilon greedy parameter\n",
    "        self.epsilon = 1\n",
    "        self.__epsilon_min = 0.01  # Minimum epsilon greedy parameter\n",
    "        self.epsilon_decay = self.__get_decay()\n",
    "\n",
    "        # # Experience replay params and buffers\n",
    "        self.__max_memory_length = 100000\n",
    "        self.replay_buffer = ReplayBuffer(self.__max_memory_length)\n",
    "\n",
    "        self.batch_size = 32\n",
    "        self.update_after_actions = 4\n",
    "\n",
    "        # update every N frames\n",
    "        self.update_target_network = 1000\n",
    "\n",
    "        self.max_steps_per_episode = 10000\n",
    "\n",
    "        # episode params\n",
    "        self.state = None\n",
    "        self.episode_start = 0\n",
    "        self.episode_reward = 0\n",
    "        self.episode_loss = []\n",
    "\n",
    "        # train stat params\n",
    "        self.frame_count = 0\n",
    "        self.episode_count = 0\n",
    "        self.running_reward = 0\n",
    "        self.running_loss = 0\n",
    "\n",
    "        self.max_reward_length = 30\n",
    "        self.episode_reward_history = deque(maxlen=self.max_reward_length)\n",
    "        self.episode_loss_history = deque(maxlen=self.max_reward_length)\n",
    "\n",
    "        # self.learning_rate = 0.00012  # rate at which NN adjusts models parameters via SGD to reduce cost\n",
    "        self.loss_function = None\n",
    "        self.optimizer = None\n",
    "\n",
    "        self.new_episode = True\n",
    "\n",
    "        np.random.seed(0)\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def epsilon_greedy_frames(self):\n",
    "        return self.__epsilon_greedy_frames\n",
    "\n",
    "    @epsilon_greedy_frames.setter\n",
    "    def epsilon_greedy_frames(self, value):\n",
    "        self.__epsilon_greedy_frames = value\n",
    "        self.epsilon_decay = self.__get_decay()\n",
    "\n",
    "    @property\n",
    "    def epsilon_min(self):\n",
    "        return self.__epsilon_min\n",
    "\n",
    "    @epsilon_min.setter\n",
    "    def epsilon_min(self, value):\n",
    "        self.__epsilon_min = value\n",
    "        self.epsilon_decay = self.__get_decay()\n",
    "\n",
    "    def __get_decay(self):\n",
    "        return (self.epsilon - self.epsilon_min) / self.epsilon_greedy_frames\n",
    "\n",
    "    @property\n",
    "    def max_memory_length(self):\n",
    "        return self.__max_memory_length\n",
    "\n",
    "    @max_memory_length.setter\n",
    "    def max_memory_length(self, value):\n",
    "        self.__max_memory_length = value\n",
    "        self.replay_buffer = ReplayBuffer(self.__max_memory_length)\n",
    "\n",
    "    def _sample_transformer(self, state):\n",
    "        if isinstance(self.env.observation_space, list):\n",
    "            # 2D samples\n",
    "            return list(map(lambda p: np.expand_dims(p, (0,1)), state))\n",
    "        else:\n",
    "            return np.expand_dims(state, (0,1))\n",
    "        \n",
    "\n",
    "    def _batch_transformer(self, batch):\n",
    "        \"\"\"работает для 1D и 2D батчей. После zip(*batch) получаем списки из фичей по размерностям\"\"\"\n",
    "        if isinstance(self.env.observation_space, list):\n",
    "            # multiple input\n",
    "            return list(map(np.array, zip(*batch)))\n",
    "        else:\n",
    "            # single input\n",
    "            shape = np.array(batch)[0].shape\n",
    "            output = np.array(batch).reshape(-1, 1, *shape)\n",
    "        return output\n",
    "\n",
    "    def act(self, state):\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if self.frame_count < self.epsilon_random_frames or self.epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            # action = np.random.choice(self.action_size) <- на CPU работает медленнее\n",
    "            action = random.sample(range(self.action_size), 1)[0]\n",
    "        else:\n",
    "            state_tensor = self._sample_transformer(state)\n",
    "            # Take best action\n",
    "            action_probs = self.model(state_tensor, training=False)\n",
    "            # action = tf.argmax(action_probs[0]).numpy() <- на CPU работает медленнее\n",
    "            action = np.argmax(action_probs[0])\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "\n",
    "        return action\n",
    "\n",
    "    def replay(self):\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        states = self._batch_transformer(np.array(states))\n",
    "        next_states = self._batch_transformer(np.array(next_states))\n",
    "\n",
    "        # Build the updated Q-values for the sampled future states\n",
    "        # Use the target model for stability\n",
    "        future_rewards = self.model_target(next_states)\n",
    "        # Q value = reward + discount factor * expected future reward\n",
    "        updated_q_values = rewards + self.gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "\n",
    "        # If final frame set the last value to -1\n",
    "        updated_q_values = updated_q_values * (1 - dones) - dones\n",
    "\n",
    "        # Create a mask so we only calculate loss on the updated Q-values\n",
    "        masks = tf.one_hot(actions, self.action_size)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Train the model on the states and updated Q-values\n",
    "            q_values = self.model(states)\n",
    "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            # Calculate loss between new Q-value and old Q-value\n",
    "            loss = self.loss_function(updated_q_values, q_action)\n",
    "            self.episode_loss.append(loss)\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "    def train(self, goal_reward=None, max_frames=None):\n",
    "\n",
    "        while True:\n",
    "            self.frame_count += 1\n",
    "\n",
    "            if self.new_episode:\n",
    "                self.new_episode = False\n",
    "                self.episode_start = time.time()\n",
    "                self.episode_loss = []\n",
    "                self.episode_reward = 0\n",
    "                self.episode_count += 1\n",
    "                self.state = self.env.reset()\n",
    "\n",
    "            # env.render()\n",
    "            action = self.act(self.state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            self.episode_reward += reward\n",
    "\n",
    "            # Save actions and states in replay buffer\n",
    "            self.replay_buffer.push(self.state, action, reward, next_state, done)\n",
    "            self.state = next_state\n",
    "\n",
    "            # Update every N frame and batch size is enough\n",
    "            if len(self.replay_buffer) > self.batch_size and self.frame_count % self.update_after_actions == 0:\n",
    "                self.replay()\n",
    "\n",
    "            # Update target network every N episodes\n",
    "            if self.frame_count % self.update_target_network == 0:\n",
    "                self.model_target.set_weights(self.model.get_weights())\n",
    "                message = f\"{datetime.now().strftime('%H:%M:%S')} Running reward: {self.running_reward:<8.2f} \" \\\n",
    "                          f\"at episode {self.episode_count:<4} | frame {self.frame_count:<6} | \" \\\n",
    "                          f\"eps: {self.epsilon:<4.2f} | Running loss: {self.running_loss:.5f}\"\n",
    "                print(message)\n",
    "\n",
    "            if done:\n",
    "                # episode is done\n",
    "                self.new_episode = True\n",
    "\n",
    "                # Update running reward to check condition for solving\n",
    "                self.episode_reward_history.append(self.episode_reward)\n",
    "                self.running_reward = np.mean(self.episode_reward_history)\n",
    "\n",
    "                self.episode_loss_history.append(np.mean(self.episode_loss))\n",
    "                self.running_loss = np.mean(self.episode_loss_history)\n",
    "\n",
    "                break\n",
    "                # Stop criteria\n",
    "                # if goal_reward is not None and self.running_reward >= goal_reward:  # Condition to consider the task solved\n",
    "                #    print(f\"Done with reward {self.running_reward} at frame {self.frame_count}\")\n",
    "                #    break\n",
    "\n",
    "            if max_frames is not None and self.frame_count >= max_frames:\n",
    "                break\n",
    "\n",
    "    def get_config(self):\n",
    "        keys = self.__dict__.keys()\n",
    "        config = {}\n",
    "        for key in keys:\n",
    "            if key in self.EXPORT_FORBIDDEN_ATTRIBUTES:\n",
    "                continue\n",
    "            else:\n",
    "                config[key] = getattr(self, key)\n",
    "        return config\n",
    "\n",
    "    def load_config(self, config):\n",
    "        keys = config.keys()\n",
    "        for key in keys:\n",
    "            if hasattr(self, key):\n",
    "                setattr(self, key, config[key])\n",
    "            else:\n",
    "                print(f\"Key {key} not found in agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed_value)\n",
    "\n",
    "core_train = core_constructor.get_core(core_config)\n",
    "env = TradeEnv(core_train, dpf_train, alias=ALIAS, log=False, log_obs=False)\n",
    "\n",
    "model = create_q_model(env)\n",
    "model_target = create_q_model(env)\n",
    "agent = DQNAgent(env, model, model_target)\n",
    "\n",
    "agent.epsilon_greedy_frames = 2000\n",
    "agent.epsilon_random_frames = int(0.05 * agent.epsilon_greedy_frames)\n",
    "agent.max_memory_length = int(1.0 * agent.epsilon_greedy_frames)\n",
    "agent.max_steps_per_episode = 50000\n",
    "agent.gamma = 0.95\n",
    "agent.epsilon_min = 0.01\n",
    "agent.batch_size = 30\n",
    "agent.update_after_actions = 4\n",
    "agent.update_target_network = 1000\n",
    "agent.loss_function = tf.keras.losses.Huber() #tf.keras.losses.MeanSquaredError()\n",
    "agent.optimizer = Adam(learning_rate=0.0005, clipnorm=0.001)    #Adam(learning_rate=learning_rate) RMSprop(learning_rate=learning_rate) SGD(learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "tp = TrainPlot4()\n",
    "core_test = core_constructor.get_core(core_config)\n",
    "tm = TrainManager(agent, core_test, dpf_test, tp, alias=ALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1639bfc44f9d4aa39151bb4e0808d23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'legendgroup': '1',\n",
       "              'line': {'color': '#109618', 'width': 1},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Train',\n",
       "              'type': 'scatter',\n",
       "              'uid': '952f393c-9a3a-489a-a388-c2c9566e6ec3',\n",
       "              'xaxis': 'x',\n",
       "              'yaxis': 'y'},\n",
       "             {'legendgroup': '1',\n",
       "              'line': {'color': '#FF9900', 'width': 1},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Test',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'afc005a4-1022-4ddd-a882-608ba3c69f5e',\n",
       "              'xaxis': 'x',\n",
       "              'yaxis': 'y'},\n",
       "             {'legendgroup': '2',\n",
       "              'line': {'color': '#D62728', 'width': 1},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Train',\n",
       "              'type': 'scatter',\n",
       "              'uid': '5b5db21e-914e-4484-b737-fb458eabc23c',\n",
       "              'xaxis': 'x2',\n",
       "              'yaxis': 'y3'},\n",
       "             {'legendgroup': '2',\n",
       "              'line': {'color': '#FF9900', 'width': 1},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Test',\n",
       "              'type': 'scatter',\n",
       "              'uid': 'f2e158f7-1686-4406-bccc-56ca6ccdaf6f',\n",
       "              'xaxis': 'x2',\n",
       "              'yaxis': 'y3'},\n",
       "             {'legendgroup': '3',\n",
       "              'line': {'color': '#1616A7', 'dash': 'dot', 'width': 1},\n",
       "              'mode': 'lines',\n",
       "              'name': 'PosTrades',\n",
       "              'type': 'scatter',\n",
       "              'uid': '591251ae-99e5-48b3-baaa-113c60503f11',\n",
       "              'xaxis': 'x3',\n",
       "              'yaxis': 'y5'},\n",
       "             {'legendgroup': '3',\n",
       "              'line': {'color': '#FB0D0D', 'dash': 'dot', 'width': 1},\n",
       "              'mode': 'lines',\n",
       "              'name': 'NegTrades',\n",
       "              'type': 'scatter',\n",
       "              'uid': '11f678ad-6cfe-4cc1-b838-edb9e1e704c3',\n",
       "              'xaxis': 'x3',\n",
       "              'yaxis': 'y5'},\n",
       "             {'legendgroup': '3',\n",
       "              'line': {'color': 'rgb(102,102,102)', 'width': 1},\n",
       "              'mode': 'lines',\n",
       "              'name': 'Sparsity',\n",
       "              'type': 'scatter',\n",
       "              'uid': '640b0791-5f78-4958-9e16-243aee22a5d3',\n",
       "              'xaxis': 'x3',\n",
       "              'yaxis': 'y6'}],\n",
       "    'layout': {'annotations': [{'font': {'size': 16},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'Trade Balance',\n",
       "                                'x': 0.47,\n",
       "                                'xanchor': 'center',\n",
       "                                'xref': 'paper',\n",
       "                                'y': 1.0,\n",
       "                                'yanchor': 'bottom',\n",
       "                                'yref': 'paper'},\n",
       "                               {'font': {'size': 16},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'Penalties',\n",
       "                                'x': 0.47,\n",
       "                                'xanchor': 'center',\n",
       "                                'xref': 'paper',\n",
       "                                'y': 0.6399999999999999,\n",
       "                                'yanchor': 'bottom',\n",
       "                                'yref': 'paper'},\n",
       "                               {'font': {'size': 16},\n",
       "                                'showarrow': False,\n",
       "                                'text': 'Num of trades by profit',\n",
       "                                'x': 0.47,\n",
       "                                'xanchor': 'center',\n",
       "                                'xref': 'paper',\n",
       "                                'y': 0.27999999999999997,\n",
       "                                'yanchor': 'bottom',\n",
       "                                'yref': 'paper'}],\n",
       "               'autosize': False,\n",
       "               'height': 800,\n",
       "               'legend': {'tracegroupgap': 180},\n",
       "               'paper_bgcolor': 'rgba(255,255,255,0)',\n",
       "               'plot_bgcolor': 'rgba(245,245,245,245)',\n",
       "               'template': '...',\n",
       "               'width': 1000,\n",
       "               'xaxis': {'anchor': 'y', 'domain': [0.0, 0.94], 'matches': 'x3', 'showticklabels': False},\n",
       "               'xaxis2': {'anchor': 'y3', 'domain': [0.0, 0.94], 'matches': 'x3', 'showticklabels': False},\n",
       "               'xaxis3': {'anchor': 'y5', 'domain': [0.0, 0.94]},\n",
       "               'yaxis': {'anchor': 'x', 'domain': [0.72, 1.0]},\n",
       "               'yaxis2': {'anchor': 'x', 'overlaying': 'y', 'side': 'right'},\n",
       "               'yaxis3': {'anchor': 'x2', 'domain': [0.36, 0.6399999999999999]},\n",
       "               'yaxis4': {'anchor': 'x2', 'overlaying': 'y3', 'side': 'right'},\n",
       "               'yaxis5': {'anchor': 'x3', 'domain': [0.0, 0.27999999999999997]},\n",
       "               'yaxis6': {'anchor': 'x3', 'overlaying': 'y5', 'range': [0, 1], 'showgrid': False, 'side': 'right'}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tp.init_plot(width=1000, height=800)\n",
    "tp.update_plot(tm.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:38:17 Running reward: -89.90   at episode 11   | frame 1000   | eps: 0.50 | Running loss: 0.77144\n",
      "23:38:31 Running reward: -61.85   at episode 21   | frame 2000   | eps: 0.01 | Running loss: 0.67022\n",
      "23:38:47 Running reward: -41.73   at episode 31   | frame 3000   | eps: 0.01 | Running loss: 0.54043\n",
      "23:39:02 Running reward: -12.13   at episode 41   | frame 4000   | eps: 0.01 | Running loss: 0.30205\n",
      "23:39:20 Running reward: -1.10    at episode 52   | frame 5000   | eps: 0.01 | Running loss: 0.10298\n",
      "23:39:38 Running reward: -1.53    at episode 62   | frame 6000   | eps: 0.01 | Running loss: 0.02445\n",
      "23:39:54 Running reward: -0.03    at episode 72   | frame 7000   | eps: 0.01 | Running loss: 0.02006\n",
      "23:40:10 Running reward: -5.37    at episode 82   | frame 8000   | eps: 0.01 | Running loss: 0.04874\n",
      "23:40:26 Running reward: -1.87    at episode 92   | frame 9000   | eps: 0.01 | Running loss: 0.07969\n",
      "23:40:41 Running reward: -0.13    at episode 103  | frame 10000  | eps: 0.01 | Running loss: 0.08713\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "tm.go(max_frames=10000, test_every=100, snapshot_every=500000, update_plot_every=100, save_since=0.06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоги\n",
    "Конфиг из предыдущей задачи не справился за 10к фреймов. \n",
    "\n",
    "- Сделал сеть шире 8х8 -> 32х32: ОК: 8к, 0.9к, 9.4к, 9.6к\n",
    "- Добавил 3 слой на 32 нейрона : ОК 1.3к, 7.4к, 1.1к, 1.1к\n",
    "\n",
    "По итогу работает таже схема - сеть погрубже и пошире и алгоритм справляется с задачей более уверенно.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rt_project_env",
   "language": "python",
   "name": "rt_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
